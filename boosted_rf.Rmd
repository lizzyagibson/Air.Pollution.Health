---
title: "boosted_rf"
author: "Ahlam Abuawad"
date: "5/2/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gbm)
library(tidyverse)
library(caret)
```

```{r include = FALSE}
data_2015 <- read_csv("data_2015.csv")
all_long <- read_csv("all_long.csv")
data_boost <- data_2015 %>% 
  select(-day_of_week, -Humidity, -date) %>% 
  as.data.frame() %>% 
  drop_na()
```


```{r train and test set}
set.seed(1)

## Creating training and test datasets
train_set = data_boost[sample(nrow(data_boost), 80),]
test_set = data_boost[!row.names(data_boost) %in% row.names(train_set),]

## Creating a matrix model for training and test sets
x_train = model.matrix(resp ~., train_set)[,-(1:2)]
y_train = train_set$resp

x_test = model.matrix(resp ~., test_set)[,-(1:2)]
y_test = test_set$resp
```


```{r boosted}
set.seed(8)

# Running boost with 100 iterations
boost.pm100 = gbm(resp ~ ., data = train_set, 
                distribution = "poisson", 
                n.trees = 100, 
                interaction.depth = 1, 
                shrinkage = 0.001) # depth should be 1 for additive model, # shrinkage is the learning rate and 0.001 is good for a small dataset and growing a lot of trees
boost.pm100
summary(boost.pm100) # ozone is extremely influential, followed by nickel and sulfur

pred.boost.train100 = predict(boost.pm100,
                         n.trees = boost.pm100$n.trees, 
                         type = "response")

pred.boost100 = predict(boost.pm100, 
                         newdata = test_set,
                         n.trees = boost.pm100$n.trees, 
                         type = "response") # this type returns counts for poisson distribution

pred.boost.round100 = round(pred.boost100)

# Finding the training set error (MSE)
boost_tmse100 = mean((pred.boost.train100 - train_set$resp)^2) 
boost_tmse100

# Finding the test set error (MSE)
boost_mse100 = mean((pred.boost100 - test_set$resp)^2) #contains true value from test data
boost_mse100

# Deviance score
dev_score100 = round(-log(dpois(test_set$resp, lambda = pred.boost100)), 2)

# Running boosted with 1000 iterations
boost.pm = gbm(resp ~ ., data = train_set, 
                distribution = "poisson", 
                n.trees = 1000, 
                interaction.depth = 1, 
                shrinkage = 0.001) # depth should be 1 for additive model, # shrinkage is the learning rate and 0.001 is good for a small dataset and growing a lot of trees
boost.pm
knitr::kable(summary(boost.pm)) # ozone is extremely influential, followed by nickel and sulfur

pred.boost.train = predict(boost.pm,
                         n.trees = boost.pm$n.trees, 
                         type = "response")

pred.boost = predict(boost.pm, 
                         newdata = test_set,
                         n.trees = boost.pm$n.trees, 
                         type = "response") # this type returns counts for poisson distribution

pred.boost.round = round(pred.boost)

# Finding the training set error (MSE)
boost_tmse = mean((pred.boost.train - train_set$resp)^2) 
boost_tmse

# Finding the test set error (MSE)
boost_mse = mean((pred.boost - test_set$resp)^2) #contains true value from test data
boost_mse

# Deviance score
dev_score = round(-log(dpois(test_set$resp, lambda = pred.boost)), 2)

# Table of first six deviance scores comparing true and predicted values from test set
compare = cbind(as.vector(test_set$resp), as.vector(pred.boost.round100), as.vector(dev_score100), as.vector(pred.boost.round), as.vector(dev_score))
rownames(compare) = colnames(test_set$resp); colnames(compare) = c("True", "Predicted for 100", "Deviance Score for 100", "Predicted for 1000", "Deviance Score for 1000")
knitr::kable(head(compare), align = "c")
```